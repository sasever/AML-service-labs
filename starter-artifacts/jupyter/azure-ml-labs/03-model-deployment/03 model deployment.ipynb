{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Model Deployment using Azure Machine Learning service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will deploy a trained model to containers using an Azure Container Instance and and Azure Kubernetes Service using the Azure Machine Learning SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will download the dataset used by this lab. Click into the following cell and use `Shift + Enter` to execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is  C:\\Users\\sasever\\Desktop\\SelfLearning\\AzureML\\AML-service-labs-master\\starter-artifacts\\jupyter\\azure-ml-labs\\03-model-deployment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UsedCars_Affordability.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "main_path = os.path.abspath(os.path.curdir)\n",
    "print(\"Current working directory is \", main_path)\n",
    "data_path = os.path.join(main_path, 'data')\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab builds upon the lessons learned in the previous lab, but is self contained so you can work thru this lab without having to run a previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read thru the following cell. Use `Shift + Enter` to execute the cell. Take a moment to look at the data loaded into the Pandas Dataframe - it contains data about used cars such as the price (in dollars), age (in years), KM (kilometers driven) and other attributes like weather it is automatic transimission, the number of doors, and the weight. \n",
    "\n",
    "In the function `train_eval_register_model` observe how the trained model is saved to the ./outputs folder along with the scaler that will be needed to scale inputs used later when scoring. Observe that we use `Model.register` to upload all files in the ./outputs folder to Azure Machine Learning as the model files. These model files will be retrieved later when the model is deployed into a container and operationalized as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.0.6\n",
      "      Age     KM  Affordable\n",
      "0      23  46986           0\n",
      "1      23  72937           0\n",
      "2      24  41711           0\n",
      "3      26  48000           0\n",
      "4      30  38500           0\n",
      "5      32  61000           0\n",
      "6      27  94612           0\n",
      "7      30  75889           0\n",
      "8      27  19700           0\n",
      "9      23  71138           0\n",
      "10     25  31461           0\n",
      "11     22  43610           0\n",
      "12     25  32189           0\n",
      "13     31  23000           0\n",
      "14     32  34131           0\n",
      "15     28  18739           0\n",
      "16     30  34000           0\n",
      "17     24  21716           0\n",
      "18     24  25563           0\n",
      "19     30  64359           0\n",
      "20     30  67660           0\n",
      "21     29  43905           0\n",
      "22     28  56349           0\n",
      "23     28  32220           0\n",
      "24     29  25813           0\n",
      "25     25  28450           0\n",
      "26     27  34545           0\n",
      "27     29  41415           0\n",
      "28     28  44142           0\n",
      "29     30  11090           0\n",
      "...   ...    ...         ...\n",
      "1406   70  44850           1\n",
      "1407   69  44826           1\n",
      "1408   80  44444           1\n",
      "1409   75  43720           1\n",
      "1410   78  43622           1\n",
      "1411   76  43532           1\n",
      "1412   69  42800           1\n",
      "1413   74  42317           1\n",
      "1414   80  42186           1\n",
      "1415   72  42000           1\n",
      "1416   79  40093           1\n",
      "1417   79  39800           1\n",
      "1418   73  39168           1\n",
      "1419   75  38945           1\n",
      "1420   76  36537           1\n",
      "1421   78  36000           1\n",
      "1422   78  36000           1\n",
      "1423   80  35821           1\n",
      "1424   73  34717           1\n",
      "1425   80  34000           1\n",
      "1426   78  30964           1\n",
      "1427   71  29000           1\n",
      "1428   72  26000           1\n",
      "1429   78  24000           1\n",
      "1430   80  23000           1\n",
      "1431   69  20544           1\n",
      "1432   72  19000           1\n",
      "1433   71  17016           1\n",
      "1434   70  16916           1\n",
      "1435   76      1           1\n",
      "\n",
      "[1436 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Load training data and define model training function\n",
    "################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import azureml\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.model import Model \n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Verify AML SDK Installed\n",
    "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
    "print(\"SDK Version:\", azureml.core.VERSION)\n",
    "\n",
    "\n",
    "# Load our training data set\n",
    "pathToCsvFile = os.path.join(data_path, 'UsedCars_Affordability.csv')\n",
    "df_affordability = pd.read_csv(pathToCsvFile, delimiter=',')\n",
    "print(df_affordability)\n",
    "\n",
    "full_X = df_affordability[[\"Age\", \"KM\"]]\n",
    "full_Y = df_affordability[[\"Affordable\"]]\n",
    "\n",
    "# Define a helper method that will train, score and register the classifier using different settings\n",
    "def train_eval_register_model(ws, experiment_name, model_name, full_X, full_Y,training_set_percentage):\n",
    "\n",
    "    # start a training run by defining an experiment\n",
    "    myexperiment = Experiment(ws, experiment_name)\n",
    "    run = myexperiment.start_logging()\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(train_X)\n",
    "    clf = linear_model.LogisticRegression(C=1)\n",
    "    clf.fit(X_scaled, train_Y)\n",
    "\n",
    "    scaled_inputs = scaler.transform(test_X)\n",
    "    predictions = clf.predict(scaled_inputs)\n",
    "    score = accuracy_score(test_Y, predictions)\n",
    "\n",
    "    print(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\n",
    "\n",
    "    # Log the training metrics to Azure Machine Learning service run history\n",
    "    run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "    run.log(\"Accuracy\", score)\n",
    "\n",
    "    # Serialize the model to a pickle file in the outputs folder\n",
    "    output_model_path = 'outputs/' + model_name + '.pkl'\n",
    "    pickle.dump(clf,open(output_model_path,'wb'))\n",
    "    print('Exported model to ', output_model_path)\n",
    "\n",
    "    # Serialize the scaler as a pickle file in the same folder as the model\n",
    "    output_scaler_path = 'outputs/' + 'scaler' + '.pkl'\n",
    "    pickle.dump(scaler,open(output_scaler_path,'wb'))\n",
    "    print('Exported scaler to ', output_scaler_path)\n",
    "\n",
    "    # notice for the model_path, we supply the name of the outputs folder without a trailing slash\n",
    "    # this will ensure both the model and the scaler get uploaded.\n",
    "    registered_model = Model.register(model_path='outputs', model_name=model_name, workspace=ws)\n",
    "\n",
    "    print(registered_model.name, registered_model.id, registered_model.version, sep = '\\t')\n",
    "\n",
    "    run.complete()\n",
    "\n",
    "    return (registered_model, clf, scaler, score, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we retrieve or create the AML Workspace and then train one instance of the model that we will deploy. In this step, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments. Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace Provisioning complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasever\\AppData\\Local\\Continuum\\anaconda3\\envs\\amlpy36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\sasever\\AppData\\Local\\Continuum\\anaconda3\\envs\\amlpy36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.50 percent of data, model accuracy reached 0.9109.\n",
      "Exported model to  outputs/usedcarsmodel.pkl\n",
      "Exported scaler to  outputs/scaler.pkl\n",
      "Registering model usedcarsmodel\n",
      "usedcarsmodel\tusedcarsmodel:4\t4\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Retrieve the AML Workspace and Train a model\n",
    "#######################################################\n",
    "\n",
    "# Create a new Workspace or retrieve the existing one\n",
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id ='757c4165-0823-49f7-9678-5a85fe5e13cc'\n",
    "\n",
    "# Provide values for the Resource Group and Workspace that will be created\n",
    "resource_group = 'MLworkspace2'\n",
    "workspace_name = 'snml2'\n",
    "workspace_region = 'westeurope'  # eastus, westcentralus, southeastasia, australiaeast, westeurope\n",
    "\n",
    "\n",
    "# By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace instead of an error\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "print(\"Workspace Provisioning complete.\")\n",
    "\n",
    "\n",
    "# Create an experiment, log metrics and register the created model\n",
    "experiment_name = \"Experiment-03-30\"\n",
    "model_name = \"usedcarsmodel\"\n",
    "training_set_percentage = 0.50\n",
    "registered_model, model, scaler, score, run = train_eval_register_model(ws, experiment_name, model_name, full_X, full_Y, training_set_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a version of a model from Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is registered with Azure Machine Learning, we can download the model files to any client and use them for scoring. In the following cell, you download the model you just registered, load both the scaler and model files retrieved by deserializing them into objects and then use them to perform a single prediction. Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model download to: azureml-models\\usedcarsmodel\\4\\outputs\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Download the registered model, re-load  the model and verify it still works\n",
    "######################################################################################\n",
    "# Download the model to a local directory\n",
    "model_path = Model.get_model_path(model_name, _workspace=ws)\n",
    "print('Model download to: ' + model_path)\n",
    "age = 60\n",
    "km = 40000\n",
    "\n",
    "# Re-load the model\n",
    "scaler = pickle.load(open(os.path.join(model_path,'scaler.pkl'),'rb'))\n",
    "scaled_input = scaler.transform([[age, km]])\n",
    "model2 = pickle.load(open(os.path.join(model_path,'usedcarsmodel.pkl'), 'rb'))\n",
    "\n",
    "# Use the loaded model to make a prediction\n",
    "prediction = model2.predict(scaled_input)\n",
    "print(prediction)\n",
    "prediction_json = json.dumps(prediction.tolist())\n",
    "print(prediction_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the container image configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you deploy a model as web service to either ACI or AKS, you are deploying a Docker container. The first steps towards deploying involve defining the contents of that container. In the following cell, you create Conda Dependencies YAML file that describes what Python packages need to be installed in the container- in this case you specify scikit-learn, numpy and pandas. Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Create a Conda dependencies environment file\n",
    "#######################################################\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "mycondaenv = CondaDependencies.create(conda_packages=['scikit-learn','numpy','pandas'])\n",
    "\n",
    "with open(\"mydeployenv.yml\",\"w\") as f:\n",
    "    f.write(mycondaenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Azure Machine Learning, you have full control over the logic of the webservice which includes how it loads your model, transforms web service inputs, uses the model for scoring and returns the result. The following cell will create a scoring web service and save the logic to the file score.py. Read thru the code that defines the webservice. Execute the following cell. The file will be deployed in the contents of the container image you are about to create in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile score.py\n",
    "scoring_service = \"\"\"\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def init():\n",
    "    try:\n",
    "        # One-time initialization of predictive model and scaler\n",
    "        from azureml.core.model import Model\n",
    "        \n",
    "        global trainedModel   \n",
    "        global scaler\n",
    "\n",
    "        model_name = \"usedcarsmodel\" \n",
    "        model_path = Model.get_model_path(model_name)\n",
    "        print('Looking for models in: ', model_path)\n",
    "\n",
    "        trainedModel = pickle.load(open(os.path.join(model_path,'usedcarsmodel.pkl'), 'rb'))\n",
    "        \n",
    "        scaler = pickle.load(open(os.path.join(model_path,'scaler.pkl'),'rb'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception during init: ', str(e))\n",
    "\n",
    "def run(input_json):     \n",
    "    try:\n",
    "        inputs = json.loads(input_json)\n",
    "\n",
    "        #Scale the input\n",
    "        scaled_input = scaler.transform(inputs)\n",
    "        \n",
    "        #Get the scored result\n",
    "        prediction = json.dumps(trainedModel.predict(scaled_input).tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        prediction = str(e)\n",
    "    return prediction\n",
    "\"\"\" \n",
    "\n",
    "with open(\"score.py\", \"w\") as file:\n",
    "    file.write(scoring_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Container Image, you need three things: the scoring script file, the runtime configuration (defining whether Python or PySpark should be used) and the Conda Dependencies file. Calling `ContainerImage.image_configuration` will capture all of the container image configuration in a single object. Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Create container image configuration\n",
    "###############################################\n",
    "# Build the ContainerImage\n",
    "runtime = \"python\" \n",
    "driver_file = \"score.py\"\n",
    "conda_file = \"mydeployenv.yml\"\n",
    "\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(execution_script = driver_file,\n",
    "                                                  runtime = runtime,\n",
    "                                                  conda_file = conda_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the container image to ACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Container Image configuration in hand, you are almost ready to deploy to ACI. The next step is to define the size of the VM that ACI will use to run your Container. Execute the following cell to create this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Create ACI configuration\n",
    "####################################\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, \n",
    "    memory_gb = 1, \n",
    "    tags = {'name':'Azure ML ACI'}, \n",
    "    description = 'This is a great example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the container that operationalizes your model as a webservice, you can use `Webservice.deploy_from_model` which will use your registered model, and automate the creation of a new Container Image, and run the created container in ACI. Execute the following cell to deploy your webservice to ACI. This step will take **10-15 minutes** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Image creation operation finished for image usedcarsmlservice01:1, operation \"Succeeded\"\n",
      "Creating service\n",
      "Running..................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# Step 7 -Deploy the webservice to ACI\n",
    "######################################\n",
    "service_name = \"usedcarsmlservice01\"\n",
    "\n",
    "webservice = Webservice.deploy_from_model(\n",
    "  workspace=ws, \n",
    "  name=service_name, \n",
    "  deployment_config=aci_config,\n",
    "  models = [registered_model], \n",
    "  image_config=image_config, \n",
    "  )\n",
    "\n",
    "webservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the webservice deployment completes, you can use the returned webservice object to invoke the webservice. Execute the following cell to invoke your webservice deployed to ACI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Test the ACI deployed webservice\n",
    "###########################################\n",
    "import json\n",
    "age = 60\n",
    "km = 40000\n",
    "test_data  = json.dumps([[age,km]])\n",
    "test_data\n",
    "result = webservice.run(input_data=test_data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the container image to AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are familiar with the process for deploying a webservice to ACI, you will find the process for deploying to AKS to be similar with one additional step that creates the AKS cluster first. Execute the following cell to provision a small AKS cluster. This step will take about **15-20 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating................................................................................................................................\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 9 - Provision an AKS cluster \n",
    "####################################\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "# Use the default configuration, overriding the default location to a known region that supports AKS\n",
    "prov_config = AksCompute.provisioning_configuration(location='westus2')\n",
    "\n",
    "aks_name = 'aks-cluster01' \n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                  provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for cluster to be ready\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your AKS cluster ready, now you can deploy your webservice. Once again, you need to provide a configuration for the size of resources allocated from the AKS cluster to run instances of your Container. Execute the following cell to deploy your webservice. This step will take **5-7 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Creating image\n",
       "Image creation operation finished for image usedcarsaksservice:5, operation &quot;Succeeded&quot;\n",
       "Creating service\n",
       "Running...............................\n",
       "SucceededAKS service creation operation finished, operation &quot;Succeeded&quot;\n",
       "Healthy\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 10 - Deploy webservice to AKS\n",
    "####################################\n",
    "# Create the web service configuration (using defaults)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "aks_service_name ='usedcarsaksservice'\n",
    "\n",
    "aks_service = Webservice.deploy_from_model(\n",
    "  workspace=ws, \n",
    "  name=aks_service_name, \n",
    "  deployment_config=aks_config,\n",
    "  models = [registered_model], \n",
    "  image_config=image_config,\n",
    "  deployment_target=aks_target\n",
    "  )\n",
    "\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, you can use the webservice object returned by the deploy_from_model method to invoke your deployed webservice. Execute the following cell to verify you can invoke the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[1]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 11 - Test the AKS deployed webservice\n",
    "############################################\n",
    "import json\n",
    "age = 60\n",
    "km = 40000\n",
    "test_data  = json.dumps([[age,km]])\n",
    "test_data\n",
    "result = aks_service.run(input_data=test_data)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "01 model deployment",
  "notebookId": 3429556884310641
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
